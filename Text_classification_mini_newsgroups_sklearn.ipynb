{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/Users/aayushkhurana/Desktop/mini_newsgroups 2')\n",
    "directories=[f for f in os.listdir('/Users/aayushkhurana/Desktop/mini_newsgroups 2') if not f.startswith('.')]  #To remove the hidden(Temporary files) files \n",
    "len(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=[\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"]\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re,string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Dataframe to split data\n",
    "l=[]\n",
    "for i in directories:\n",
    "    #print(i)\n",
    "    os.chdir('/Users/aayushkhurana/Desktop/mini_newsgroups 2/'+i)\n",
    "    for j in os.listdir():\n",
    "         l.append(['/Users/aayushkhurana/Desktop/mini_newsgroups 2/'+i+'/'+j,i])\n",
    "            \n",
    "df = pd.DataFrame(l, columns = ['Path', 'Class_name'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 2), (500, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train,df_test=train_test_split(df,random_state=1)\n",
    "df_train.shape,df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 2), (500, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=np.array(df_train)\n",
    "x_test=np.array(df_test)\n",
    "x_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits=set(string.digits)\n",
    "regex = re.compile('[%s]' % re.escape(string.digits))\n",
    "punctuations=set(string.punctuation)\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={}\n",
    "for i in range(0,x_train.shape[0]):\n",
    "    list=[]\n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    list3=[]\n",
    "    list4=[]\n",
    "    f=open(x_train[i][0],'r',errors=\"ignore\")\n",
    "    r=f.readlines()\n",
    "    list=\" \".join(r[7:]).lower().split()              \n",
    "    list1=\" \".join(list)  \n",
    "    list2=''.join(ch for ch in list1 if ch not in digits)                  #Removal of Digits\n",
    "    list3=''.join(ch for ch in list2 if ch not in punctuations)            #Removal of Punctuations\n",
    "    list3=list3.split()\n",
    "    for i in range(0,len(list3)):                                          #Removal of Stop Words\n",
    "        if list3[i] in stopwords:\n",
    "            list3[i]=\"\"\n",
    "    list4=\" \".join(list3)\n",
    "    list4=list4.split()\n",
    "    for  i in list4:                                                        #Creating the dictionary of all the words \n",
    "        if i in d:\n",
    "            d[i]=d[i]+1\n",
    "        else:\n",
    "            d[i]=1\n",
    "#print(d,end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-17adc45e2a17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m#Sorting the dictionary according to its values in descending order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdict1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdict1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdict1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "dict=sorted(d.items(),key=lambda x:x[1],reverse=True)     #Sorting the dictionary according to its values in descending order\n",
    "dict1=np.array(dict)\n",
    "dict1.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=np.zeros((1500,3000))        #Creating an empty numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 3000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.DataFrame(df1,columns=dict1[0:3000,0])     #Creating an empty dataframe \n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list=[]\n",
    "index=0\n",
    "for i in range(0,x_train.shape[0]):\n",
    "    y_list.append(x_train[i][1])\n",
    "    list=[]\n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    list3=[]\n",
    "    list4=[]\n",
    "    f=open(x_train[i][0],'r',errors=\"ignore\")\n",
    "    r=f.readlines()\n",
    "    list=\" \".join(r[7:]).lower().split()\n",
    "    list1=\" \".join(list) \n",
    "    list2=''.join(ch for ch in list1 if ch not in digits)\n",
    "    list3=''.join(ch for ch in list2 if ch not in punctuations)\n",
    "    list3=list3.split()\n",
    "    for i in range(0,len(list3)):\n",
    "        if list3[i] in stopwords:\n",
    "            list3[i]=\"\"\n",
    "    list4=\" \".join(list3)\n",
    "    list4=list4.split()\n",
    "    for word in list4:\n",
    "        if word in df2.columns:\n",
    "            df2[word][index]=df2[word][index]+1\n",
    "    index=index+1\n",
    "    \n",
    "   # print(list4,end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2                #df3 is the dataframe containing all the training data values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500,), (1500, 3000))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=np.array((y_list))\n",
    "y_train.shape,df2.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=np.zeros((500,3000))\n",
    "df5=pd.DataFrame(df4,columns=dict1[0:3000,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list1=[]\n",
    "index=0\n",
    "for i in range(0,x_test.shape[0]):\n",
    "    y_list1.append(x_test[i][1])\n",
    "    list=[]\n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    list3=[]\n",
    "    list4=[]\n",
    "    f=open(x_test[i][0],'r',errors=\"ignore\")\n",
    "    r=f.readlines()\n",
    "    list=\" \".join(r[7:]).lower().split()\n",
    "    list1=\" \".join(list) \n",
    "    list2=''.join(ch for ch in list1 if ch not in digits)\n",
    "    list3=''.join(ch for ch in list2 if ch not in punctuations)\n",
    "    list3=list3.split()\n",
    "    for i in range(0,len(list3)):\n",
    "        if list3[i] in stopwords:\n",
    "            list3[i]=\"\"\n",
    "    list4=\" \".join(list3)\n",
    "    list4=list4.split()\n",
    "    for word in list4:\n",
    "        if word in df5.columns:\n",
    "            df5[word][index]=df5[word][index]+1\n",
    "    index=index+1\n",
    "    \n",
    " #   print(list4,end=\"\\n\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=df5       #d6 is the dataframe containing the testing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(df3.values,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(df5.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, (500,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=np.array(df_test.iloc[:,1])\n",
    "len(y_pred),y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  0,  0,  0,  0,  0,  0,  1,  3,  0,  0,  0,  0,  0,  0,  2,\n",
       "         1,  2,  1,  7],\n",
       "       [ 0,  9,  7,  3,  1,  4,  3,  0,  0,  2,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  1],\n",
       "       [ 0,  1, 13,  5,  2,  3,  0,  0,  1,  1,  1,  0,  1,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  0,  2, 11,  3,  0,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  1,  4,  1, 19,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  4,  2,  0,  2, 11,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  0,  1,  1,  0,  0, 17,  1,  0,  0,  0,  0,  1,  0,  1,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  1,  1,  0,  0, 19,  2,  0,  0,  0,  0,  0,  0,  0,\n",
       "         1,  0,  0,  0],\n",
       "       [ 0,  0,  1,  0,  0,  0,  0,  3, 22,  1,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  0, 17,  1,  0,  0,  0,  0,  0,\n",
       "         0,  1,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  1, 25,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  1,  1,  0,  1,  0,  0,  1,  0, 16,  0,  0,  0,  0,\n",
       "         1,  0,  0,  0],\n",
       "       [ 0,  1,  2,  5,  1,  1,  1,  4,  2,  0,  0,  1,  4,  0,  0,  0,\n",
       "         1,  0,  0,  1],\n",
       "       [ 1,  1,  1,  0,  0,  0,  0,  3,  1,  1,  0,  0,  0, 16,  1,  0,\n",
       "         0,  0,  0,  1],\n",
       "       [ 0,  1,  0,  0,  0,  0,  0,  2,  0,  1,  1,  2,  1,  0, 23,  0,\n",
       "         2,  0,  2,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,  1,  0,  0,  0,  0,\n",
       "        14,  0,  2,  1],\n",
       "       [ 2,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
       "         0, 18,  1,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         8,  0, 16,  4],\n",
       "       [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  2,\n",
       "         5,  0,  2, 10]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_pred=y_pred,y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.46      0.39      0.42        28\n",
      "           comp.graphics       0.50      0.30      0.37        30\n",
      " comp.os.ms-windows.misc       0.39      0.46      0.43        28\n",
      "comp.sys.ibm.pc.hardware       0.38      0.58      0.46        19\n",
      "   comp.sys.mac.hardware       0.63      0.70      0.67        27\n",
      "          comp.windows.x       0.58      0.52      0.55        21\n",
      "            misc.forsale       0.68      0.77      0.72        22\n",
      "               rec.autos       0.50      0.76      0.60        25\n",
      "         rec.motorcycles       0.67      0.81      0.73        27\n",
      "      rec.sport.baseball       0.68      0.85      0.76        20\n",
      "        rec.sport.hockey       0.89      0.93      0.91        27\n",
      "               sci.crypt       0.76      0.73      0.74        22\n",
      "         sci.electronics       0.57      0.17      0.26        24\n",
      "                 sci.med       1.00      0.62      0.76        26\n",
      "               sci.space       0.92      0.64      0.75        36\n",
      "  soc.religion.christian       0.80      1.00      0.89        20\n",
      "      talk.politics.guns       0.42      0.70      0.53        20\n",
      "   talk.politics.mideast       0.86      0.78      0.82        23\n",
      "      talk.politics.misc       0.67      0.55      0.60        29\n",
      "      talk.religion.misc       0.38      0.38      0.38        26\n",
      "\n",
      "               micro avg       0.62      0.62      0.62       500\n",
      "               macro avg       0.64      0.63      0.62       500\n",
      "            weighted avg       0.64      0.62      0.62       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred=y_pred,y_true=y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
