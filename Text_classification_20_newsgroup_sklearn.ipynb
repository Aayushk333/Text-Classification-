{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the libraries that will be required in the project\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re,string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "os.chdir('/Users/aayushkhurana/Desktop/20_newsgroups')\n",
    "directories=[f for f in os.listdir('/Users/aayushkhurana/Desktop/20_newsgroups') if not f.startswith('.')]  #To remove the hidden(Temporary files) files \n",
    "len(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of Stop Words\n",
    "stopwords=[\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"]\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19997, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Dataframe to split data into train and test\n",
    "l=[]\n",
    "for i in directories:\n",
    "    #print(i)\n",
    "    os.chdir('/Users/aayushkhurana/Desktop/20_newsgroups/'+i)\n",
    "    for j in os.listdir():\n",
    "         l.append(['/Users/aayushkhurana/Desktop/20_newsgroups/'+i+'/'+j,i])\n",
    "            \n",
    "df = pd.DataFrame(l, columns = ['Path', 'Class_name'])          #Dataframe has been created\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14997, 2), (5000, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting data into training and testing data\n",
    "df_train,df_test=train_test_split(df,random_state=1)\n",
    "df_train.shape,df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14997, 2), (5000, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a numpy array of the dataframe\n",
    "x_train=np.array(df_train)\n",
    "x_test=np.array(df_test)\n",
    "x_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of digits and punctuations\n",
    "digits=set(string.digits)\n",
    "regex = re.compile('[%s]' % re.escape(string.digits))\n",
    "punctuations=set(string.punctuation)\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traversing through all the training files of all the classes(directories) and creating a dictionary of all the words that appear in these files along with their frequencies\n",
    "d={}\n",
    "for i in range(0,x_train.shape[0]):\n",
    "    list=[]\n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    list3=[]\n",
    "    list4=[]\n",
    "    f=open(x_train[i][0],'r',errors=\"ignore\")                             #Opening the file in reading mode\n",
    "    r=f.readlines()\n",
    "    list=\" \".join(r[7:]).lower().split()              \n",
    "    list1=\" \".join(list)  \n",
    "    list2=''.join(ch for ch in list1 if ch not in digits)                  #Removal of Digits\n",
    "    list3=''.join(ch for ch in list2 if ch not in punctuations)            #Removal of Punctuations\n",
    "    list3=list3.split()\n",
    "    for i in range(0,len(list3)):                                          #Removal of Stop Words\n",
    "        if list3[i] in stopwords:\n",
    "            list3[i]=\"\"\n",
    "    list4=\" \".join(list3)\n",
    "    list4=list4.split()\n",
    "    for  i in list4:                                                        #Creating the dictionary of all the words \n",
    "        if i in d:\n",
    "            d[i]=d[i]+1\n",
    "        else:\n",
    "            d[i]=1\n",
    "#print(d,end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137290, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sorting the dictionary according to its values in descending order\n",
    "dict=sorted(d.items(),key=lambda x:x[1],reverse=True)     \n",
    "dict1=np.array(dict)\n",
    "dict1.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=np.zeros((14997,4000))        #Creating an empty numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14997, 4000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.DataFrame(df1,columns=dict1[0:4000,0])     #Creating an empty dataframe \n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling the Dataframe with value of frequencies of words that appear in the training files\n",
    "y_list=[]\n",
    "index=0\n",
    "for i in range(0,x_train.shape[0]):\n",
    "    y_list.append(x_train[i][1])\n",
    "    list=[]\n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    list3=[]\n",
    "    list4=[]\n",
    "    f=open(x_train[i][0],'r',errors=\"ignore\")\n",
    "    r=f.readlines()\n",
    "    list=\" \".join(r[7:]).lower().split()\n",
    "    list1=\" \".join(list) \n",
    "    list2=''.join(ch for ch in list1 if ch not in digits)\n",
    "    list3=''.join(ch for ch in list2 if ch not in punctuations)\n",
    "    list3=list3.split()\n",
    "    for i in range(0,len(list3)):\n",
    "        if list3[i] in stopwords:\n",
    "            list3[i]=\"\"\n",
    "    list4=\" \".join(list3)\n",
    "    list4=list4.split()\n",
    "    for word in list4:\n",
    "        if word in df2.columns:\n",
    "            df2[word][index]=df2[word][index]+1\n",
    "    index=index+1\n",
    "    \n",
    "   # print(list4,end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2                #df3 is the dataframe containing all the training data values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14997,), (14997, 4000))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=np.array((y_list))\n",
    "y_train.shape,df2.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=np.zeros((5000,4000))                      #Creating an empty numpy array with all zeroes\n",
    "df5=pd.DataFrame(df4,columns=dict1[0:4000,0])  #Creating the DataFrame with columns  as the top frequency words of the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling the Dataframe with value of frequencies of words that appear in the testing files\n",
    "y_list1=[]\n",
    "index=0\n",
    "for i in range(0,x_test.shape[0]):\n",
    "    y_list1.append(x_test[i][1])\n",
    "    list=[]\n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    list3=[]\n",
    "    list4=[]\n",
    "    f=open(x_test[i][0],'r',errors=\"ignore\")\n",
    "    r=f.readlines()\n",
    "    list=\" \".join(r[7:]).lower().split()\n",
    "    list1=\" \".join(list) \n",
    "    list2=''.join(ch for ch in list1 if ch not in digits)\n",
    "    list3=''.join(ch for ch in list2 if ch not in punctuations)\n",
    "    list3=list3.split()\n",
    "    for i in range(0,len(list3)):\n",
    "        if list3[i] in stopwords:\n",
    "            list3[i]=\"\"\n",
    "    list4=\" \".join(list3)\n",
    "    list4=list4.split()\n",
    "    for word in list4:\n",
    "        if word in df5.columns:\n",
    "            df5[word][index]=df5[word][index]+1\n",
    "    index=index+1\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=df5       #d6 is the dataframe containing the testing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000,), (5000, 4000))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=np.array(y_list1)\n",
    "x_test=df6.values\n",
    "y_test.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=MultinomialNB()        #Creating an object of MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(df3.values,y_train)  #Fitting the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(df5.values)   #Making the predictions on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[198,   0,   0,   0,   0,   0,   1,   2,   4,   2,   0,   1,   1,\n",
       "          5,   3,   2,   3,   1,   1,  36],\n",
       "       [  0, 157,  16,  14,   7,  14,  10,   3,   4,   0,   1,   0,   5,\n",
       "          5,   3,   0,   0,   0,   0,   0],\n",
       "       [  0,  15, 184,  29,   7,  13,   6,   1,   1,   1,   0,   0,   6,\n",
       "          0,   1,   0,   0,   0,   1,   0],\n",
       "       [  0,   7,  16, 159,  30,   0,  10,   1,   1,   0,   0,   2,   8,\n",
       "          1,   0,   0,   0,   0,   0,   1],\n",
       "       [  0,   2,   5,  16, 212,   1,   9,   2,   2,   0,   0,   0,   3,\n",
       "          1,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,  37,  13,   3,   2, 178,   3,   0,   3,   1,   0,   0,   4,\n",
       "          2,   2,   0,   0,   0,   0,   0],\n",
       "       [  0,   4,   4,  18,  10,   0, 175,   2,   8,   0,   2,   1,   2,\n",
       "          0,   1,   0,   0,   0,   1,   0],\n",
       "       [  0,   5,   0,   1,   2,   0,   7, 207,  16,   1,   0,   0,   4,\n",
       "          1,   0,   0,   2,   0,   2,   0],\n",
       "       [  1,   2,   0,   0,   1,   0,   4,   5, 228,   0,   1,   0,   1,\n",
       "          0,   0,   0,   3,   0,   1,   0],\n",
       "       [  0,   1,   0,   0,   0,   1,  10,   3,   2, 238,   4,   1,   1,\n",
       "          2,   0,   0,   1,   0,   0,   2],\n",
       "       [  0,   0,   0,   0,   0,   0,   2,   1,   5,  11, 230,   0,   1,\n",
       "          0,   0,   0,   0,   0,   1,   1],\n",
       "       [  0,   7,   2,   3,   2,   2,   0,   1,   2,   0,   0, 186,   4,\n",
       "          0,   0,   0,   5,   1,   7,   1],\n",
       "       [  0,   9,   3,   6,  15,   1,  12,   6,   4,   1,   2,   4, 170,\n",
       "          1,   2,   0,   1,   0,   1,   0],\n",
       "       [  6,   5,   0,   0,   1,   2,   3,   4,   9,   3,   1,   1,   8,\n",
       "        234,   3,   0,   4,   0,   7,   2],\n",
       "       [  1,   7,   3,   1,   1,   0,   1,   2,   7,   2,   0,   0,   6,\n",
       "          2, 216,   0,   0,   0,   5,   1],\n",
       "       [  2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0, 251,   0,   1,   2,   0],\n",
       "       [  1,   0,   1,   1,   2,   0,   3,   1,   4,   0,   0,   1,   4,\n",
       "          0,   1,   0, 198,   1,  14,   3],\n",
       "       [  4,   1,   0,   1,   0,   0,   3,   3,   3,   0,   0,   2,   0,\n",
       "          1,   0,   2,   2, 215,  20,   4],\n",
       "       [ 10,   0,   0,   0,   0,   0,   3,   2,   6,   4,   0,   3,   0,\n",
       "          1,   3,   1,  41,   4, 150,  17],\n",
       "       [ 73,   0,   0,   0,   0,   0,   0,   0,   2,   0,   0,   3,   2,\n",
       "          3,   3,  21,  24,   1,  21,  99]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing the Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_pred=y_pred,y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.67      0.76      0.71       260\n",
      "           comp.graphics       0.61      0.66      0.63       239\n",
      " comp.os.ms-windows.misc       0.74      0.69      0.72       265\n",
      "comp.sys.ibm.pc.hardware       0.63      0.67      0.65       236\n",
      "   comp.sys.mac.hardware       0.73      0.84      0.78       253\n",
      "          comp.windows.x       0.84      0.72      0.77       248\n",
      "            misc.forsale       0.67      0.77      0.71       228\n",
      "               rec.autos       0.84      0.83      0.84       248\n",
      "         rec.motorcycles       0.73      0.92      0.82       247\n",
      "      rec.sport.baseball       0.90      0.89      0.90       266\n",
      "        rec.sport.hockey       0.95      0.91      0.93       252\n",
      "               sci.crypt       0.91      0.83      0.87       223\n",
      "         sci.electronics       0.74      0.71      0.73       238\n",
      "                 sci.med       0.90      0.80      0.85       293\n",
      "               sci.space       0.91      0.85      0.88       255\n",
      "  soc.religion.christian       0.91      0.98      0.94       256\n",
      "      talk.politics.guns       0.70      0.84      0.76       235\n",
      "   talk.politics.mideast       0.96      0.82      0.89       261\n",
      "      talk.politics.misc       0.64      0.61      0.63       245\n",
      "      talk.religion.misc       0.59      0.39      0.47       252\n",
      "\n",
      "               micro avg       0.78      0.78      0.78      5000\n",
      "               macro avg       0.78      0.78      0.77      5000\n",
      "            weighted avg       0.78      0.78      0.78      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred=y_pred,y_true=y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
